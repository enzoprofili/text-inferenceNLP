---
title: "Text Inference"
author: "Enzo Profli"
output:
  bookdown::pdf_book:
    fig_caption: yes
    keep_tex: yes
    toc: false
    number_sections: true
header-includes: 
    \usepackage{graphicx}
    \usepackage{float} 
    \floatplacement{figure}{H}
urlcolor: cyan
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
library(reticulate)
library(stringr)
library(readr)
library(stringi)
library(tokenizers)
library(babynames)
library(maps)
library(dplyr)
library(tm)
library(pROC)
```

```{r cleaning, include = FALSE}
# correct files for reading (r crashes if weird characters are not corrected)
filenames = dir("./BI-articles/2013")

for (i in filenames){
  test_file <- read_file(paste0("./BI-articles/2013/", i)) 
  test_file <- iconv(test_file, "UTF-8", "UTF-8", sub='')
  write.table(test_file, paste0("./BI-articles/2013/", i))
} 

filenames = dir("./BI-articles/2014")

for (i in filenames){
  test_file <- read_file(paste0("./BI-articles/2014/", i)) 
  test_file <- iconv(test_file, "UTF-8", "UTF-8", sub='')
  write.table(test_file, paste0("./BI-articles/2014/", i))
} 

test_file <- gsub(",", "", test_file)
test_file <- tokenize_sentences(test_file)

# read in CEO data
ceos <- read.csv("./labels/labels/ceo.csv", sep = ";", header = F)

## keep only full names (through inspection only full CEO names are given)
ceos$V1 <- gsub(",$", "", ceos$V1)
ceos$V1 <- gsub(",", " ", ceos$V1)
ceos <- as.vector(ceos[grep(" ", ceos$V1), ])

# load in cities dataset
data(world.cities)

# load in US names dataset and filter for common names
common_names <- babynames %>%
                  group_by(name) %>%
                  summarise(occurences = sum(n)) %>%
                  filter(occurences > 5000)

# read in and clean company labels
incs <- read.csv("./labels/labels/companies.csv", sep = ";", header = F)
incs$V1 <- gsub("[Ii]nc|[Cc]o$|Group|Ltd|Corp.*|Manag.*|^The ", "", incs$V1)
incs$V1 <- trimws(incs$V1)
incs$V1 <- ifelse(is.na(word(incs$V1, 1, 2)), incs$V1, word(incs$V1, 1, 2))
incs <- incs %>% distinct(V1)

```

```{python pysetup, include = FALSE}
import pandas as pd
import nltk
import re
from nltk.corpus import stopwords
import os
```

```{python patterns, include = FALSE}
os.getcwd()
os.chdir('C:\\Users\\enzop\\Desktop\\Enzo\\Northwestern\\IEMS308\\HW3\\BI-articles\\2013') #set your path

#use 2013 data as training
event_list = []

#find each sequence of two consecutive capitalized words and extract them: output sentence (3 words before/after) and name
for filename in os.listdir(os.getcwd()):
  text = open(filename,"r").read()
  tuple_list = re.findall(r'((?:\S+\s+){0,3}\b([A-Z]+[a-z]*(?=\s[A-Z])(?:\s[A-Z]+[a-z]*){1})\b\s*(?:\S+\b\s*){0,3})', text)
  event_list.append(tuple_list)

#unlist items and add tuples to dataframe
events = [item for sublist in event_list for item in sublist]
events = pd.DataFrame(events, columns=["Sentence", "Person"])


#repeat for 2014 data (will be used for testing)
event_list = []
os.chdir('C:\\Users\\enzop\\Desktop\\Enzo\\Northwestern\\IEMS308\\HW3\\BI-articles\\2014') #set your path

#find each sequence of two consecutive capitalized words and extract them: output sentence (3 words before/after) and name
for filename in os.listdir(os.getcwd()):
  text = open(filename,"r").read()
  tuple_list = re.findall(r'((?:\S+\s+){0,3}\b([A-Z]+[a-z]*(?=\s[A-Z])(?:\s[A-Z]+[a-z]*){1})\b\s*(?:\S+\b\s*){0,3})', text)
  event_list.append(tuple_list)

#unlist items and add tuples to dataframe
events_2014 = [item for sublist in event_list for item in sublist]
events_2014 = pd.DataFrame(events_2014, columns=["Sentence", "Person"])

os.chdir('C:\\Users\\enzop\\Desktop\\Enzo\\Northwestern\\IEMS308\\HW3')
```

# CEOs

The first Name Entity Recognition (NER) model was run to assess CEO names in the Business Insider data. The logistic regression model used 2013 articles as a training dataset, and then subsequently tested on 2014 data - the data was segmented into sentences to allow for easier matching. In business journalism (and by inspection), CEOs are usually denominated using a \textit{FirstName LastName} approach. Thus, the CEO labels were altered to match this pattern (without commas, or \textit{LastName FirstName}, etc.). Moreover, the datasets were generated by filtering for consecutive capitalized words, and they have two columns: a Sentence column containing the match plus 3 preceeding words and 3 word succeeding words, and a Person column containing the match. If the Person column matches any of the CEO labels, they are assigned as a ceo = 1 variable, and 0 otherwise. Finally, the re.findall function in Python was used to match the following regex in the data: '((?:\\S+\\s+)\{0,3\}\\b([A-Z]+[a-z]\*(?=\\s[A-Z])(?:\\s[A-Z]+[a-z]\*)\{1\})\\b\\s\*(?:\\S+\\b\\s*)\{0,3\})'.

Given this structure, features were generated to train the model, based on Sentence column string patterns:

* name: is the first capitalized word a common name? To produce this feature, the babynames dataset in R was utilized to generate a complete list of babynames, using Census data.

* location: does the column match a location? The world.cities dataset in R was utilized to produce matches.

* location2: do any of the words in Person contain a location identifier? Examples: Street, Place, Square, River, etc.

* company: any company identifier? Examples: Corp, Ltd, etc.

* company2: is there a company label in the sentence? This feature was generated by matching with provided company labels.

* dow: any day of the week identifier in Person?

* n_capitalized: number of capitalized words in sentence.

* denomination: CEO name usually followed by position description (CEO, chief, leader, president, billionaire, etc.). Are there any of those identifiers in the sentence?

Given these features, the model was trained, with good results. The model's area under the curve (AUC) was 0.8, and the ROC curve can be seen below in Figure \@ref(fig:CEOmodel). The highest probability matches (over a .2 probability match in the 2014 data) can be seen in ceo_matches.csv.

```{r predictors}
events = py$events

# create feature dataset
events <- events %>% 
            mutate(ceo = ifelse(Person %in% ceos, 1, 0),                                                             #is present in CEO label? (response variable)
                   first_name = gsub( " .*$", "", Person), 
                   second_name = gsub( "^.* ", "", Person),
                   name = ifelse(first_name %in% common_names$name, 1, 0),                                           #is first capitalized word a somewhat common name?
                   location = ifelse(Person %in% world.cities$name | Person %in% world.cities$country.etc, 1, 0),    #is it a location?
                   location2 = ifelse(grepl("River|Street|Place|Land|Drive|Avenue|Valley|City|Town", Person), 1, 0), #does it have place identifiers?
                   company = ifelse(grepl("Corp|Inc|Ltd|Management", Person), 1, 0),                                 #does it have company identifiers?
                   company2 = ifelse(first_name %in% incs$V1, 1, 0),                             #does first name match with a company?
                   dow = ifelse(grepl("Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday", Person), 1, 0),    #does the name match a day of the week?
                   n_capitalized = str_count(Sentence, "[A-Z]"),                                                     #number of capitalized words
                   denomination = ifelse(grepl("CEO|[Dd]irector|[Cc]hair|[Ff]ounder|[Pp]resident|[Bb]usinessman|[Cc]hief [Ee]xec|[Bb]oss|[Bb]illionaire|[Vv]eteran|[Ll]eader|[Tt]ycoon|[Tt]itan|[Mm]anager", Sentence), 1, 0)) %>%            #does the sentence mention a leadership position?
            arrange(Person)
```

```{r CEOmodel, fig.cap = "ROC Curve for CEO NER model"}
# run logistic regression
logit <- glm(ceo ~ name + location + location2 + company + company2 + dow + n_capitalized + denomination, data = events, family = "binomial")

# set up test data (same as for 2013 data)
events_2014 <- py$events_2014
events_2014 <- events_2014 %>% 
                  mutate(ceo = ifelse(Person %in% ceos, 1, 0),
                         first_name = gsub( " .*$", "", Person),
                         second_name = gsub( "^.* ", "", Person),
                         name = ifelse(first_name %in% common_names$name, 1, 0),
                         location = ifelse(Person %in% world.cities$name | Person %in% world.cities$country.etc, 1, 0),
                         location2 = ifelse(grepl("River|Street|Place|Land|Drive|Avenue", Person), 1, 0),
                         company = ifelse(grepl("Corp|Inc|Ltd|Management", Person), 1, 0),
                         company2 = ifelse(first_name %in% incs$V1, 1, 0),
                         dow = ifelse(grepl("Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday", Person), 1, 0),
                         n_capitalized = str_count(Sentence, "[A-Z]"),
                         denomination = ifelse(grepl("CEO|[Dd]irector|[Cc]hair|[Ff]ounder|[Pp]resident|[Bb]usinessman|[Cc]hief [Ee]xec|[Bb]oss|[Bb]illionaire|[Vv]eteran|[Ll]eader|[Tt]ycoon|[Tt]itan|[Mm]anager", Sentence), 1, 0)) %>%
                  arrange(Person)

# run predictions on test data
events_2014$predictions <- predict(logit, events_2014, type = "response")

# AUC analysis
roc_obj <- roc(events_2014$ceo, events_2014$predictions)
auc(roc_obj)
plot(roc_obj)

# return predicted CEO names (arbitrary probability cutoff: 0.2)
predicted_CEOs <- events_2014 %>%
                    filter(predictions > 0.2) %>%
                    arrange(-predictions) %>%
                    distinct(Person)
write.csv(predicted_CEOs, "ceo_matches.csv")
```

# Companies

A similar approach was conducted to find companies, with slightly lower performance. Only the first two names of company labels were kept, and words such as Co, Corp, Group, etc. were removed. Companies may have one or multiple names, and so the regex was altered to match this situation: ((?:\\S+\\s+)\{0,3\}[^\\.]\\b(?:[A-Z][a-z]\*\\s+)+\\b\\s\*(?:\\S+\\b\\s*){0,3}) - note that we still keep words around the match, as with CEOs. We trained the model with similar variables as above, but with some additions:

* stock: in these articles, companies are usually mentioned in the context of the stock market. Does the sentence allude to stocks?

* verbs: similar to stock, but with verb stems (earn, jump, stumble, etc.).

* political: a lot of articles are related to politics, and to filter these out, we ask: are there politically-related words in the sentence?

* preposition: by inspection, many company names are preceded by prepositions (ex: "at Tesla, engineers...", or "talks with Tesla have broken down"). Do any prepositions precede the potential company name?

* possession/comma: similar to preposition, but with possession identifiers (ex: "Tesla's IPO") and commas (ex: "Tesla, whose shares jumped,...")

With these variables, the model achieved an AUC of .69, which means the NER model is somewhat predictive, but prone to errors. By inspection, companies are mistaken for countries, which possibly relates to the fact that the labels themselves are not perfect (for example, "United States" is labeled as a company, and thus countries/nationalities are probably mistaken for companies). Figure \@ref(fig:corpmodel) displays the ROC curve for this model, and company_matches.csv shows matches that were considered very likely to be companies (probability > 0.35).

```{python companypatterns, include = FALSE}
os.getcwd()
os.chdir('C:\\Users\\enzop\\Desktop\\Enzo\\Northwestern\\IEMS308\\HW3\\BI-articles\\2013') #set your path

#use 2013 data as training
company_list = []

#find each capitalized word (not after period) and extract them: output sentence (3 words before/after) and name
for filename in os.listdir(os.getcwd()):
  text = open(filename,"r").read()
  tuple_list = re.findall(r'((?:\S+\s+){0,3}[^\.]\b(?:[A-Z][a-z]*\s+)+\b\s*(?:\S+\b\s*){0,3})', text)
  company_list.append(tuple_list)

#unlist items and add tuples to dataframe
company = [item for sublist in company_list for item in sublist]
company = pd.DataFrame(company, columns=["Sentence"])

#repeat for 2014 data (will be used for testing)
company_list = []
os.chdir('C:\\Users\\enzop\\Desktop\\Enzo\\Northwestern\\IEMS308\\HW3\\BI-articles\\2014') #set your path

#find each sequence of two consecutive capitalized words and extract them: output sentence (3 words before/after) and name
for filename in os.listdir(os.getcwd()):
  text = open(filename,"r").read()
  tuple_list = re.findall(r'((?:\S+\s+){0,3}[^\.]\b(?:[A-Z][a-z]*\s+)+\b\s*(?:\S+\b\s*){0,3})', text)
  company_list.append(tuple_list)

#unlist items and add tuples to dataframe
company_2014 = [item for sublist in company_list for item in sublist]
company_2014 = pd.DataFrame(company_2014, columns=["Sentence"])
```

```{r corppredictors}
#extract py dataframe to R
companies = data.frame(py$company)
names(companies) = "Sentence"
companies$Company <- sapply(str_extract_all(companies$Sentence, "\\b[A-Z]+[a-z]*\\b"), paste, collapse= ' ')
companies$Company <- trimws(companies$Company)

#filter stopwords in Company column and small names
companies <- companies %>% filter(!(tolower(Company) %in% stopwords(kind = "en")) & 
                                    nchar(Company) > 3 & 
                                    !(grepl("[.,!?]", Company)) &
                                    !(grepl("America", Company))&
                                    str_count(Company, "\\s") < 3) #remove stop words, small names and non-letter character potential companies
companies$Company <- gsub("'s*|,$", "", companies$Company) #remove possessives or commas

#add predictors
companies <- companies %>%
                mutate(correct = ifelse((Company %in% incs$V1 & !(Company %in% world.cities$country.etc)), 1, 0),       #is name represented in labels dataset?
                       board = ifelse(grepl("board|executive|manager|founder|chief|business|veteran", Sentence), 1, 0), #business identifiers
                       stock = ifelse(grepl("stock|shares|performance|products", Sentence), 1, 0),                      #stock identifiers
                       verbs = ifelse(grepl("jump|rose|trad|rall|announc|spen|earn|slump|fall|miss", Sentence), 1, 0),  #company verb identifiers
                       location = ifelse(Company %in% world.cities$name | Company %in% world.cities$country.etc, 1, 0),
                       location2 = ifelse(grepl("River|Street|Place|Land|Drive|Avenue", Company), 1, 0),
                       name = ifelse(Company %in% common_names$name, 1, 0),
                       company = ifelse(grepl("Corp|Inc|Ltd|Management", Company), 1, 0),
                       dow = ifelse(grepl("Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday", Company), 1, 0),
                       month = ifelse(grepl("January|February|March|April|May|June|July|August|September|October|November|December", Company), 1, 0),
                       n_capitalized = str_count(Sentence, "[A-Z]"),
                       n_words = str_count(Sentence, "\\s"),
                       political = ifelse(grepl(" House| Representative| Republican| Democrat| Speaker| Congress| Senate| The | Fiscal| Congress| Central| Bank", Sentence), 1, 0))

#add final predictors
companies <- companies %>%
                rowwise() %>%
                mutate(preposition = ifelse(grepl(paste0("( to| with| at| by| from) ", Company), Sentence), 1, 0),
                       possession = ifelse(grepl(paste0(Company, "\'s"), Sentence), 1, 0),
                       comma = ifelse(grepl(paste0(Company, ","), Sentence), 1, 0))
```

```{r corpmodel, fig.cap = "ROC Curve for company NER model"}
# run logistic regression
logit <- glm(correct ~ board + stock + verbs + name + location + location2 + company + dow + month + n_capitalized + n_words + political + preposition + possession + comma, data = companies, family = "binomial")

#extract py dataframe to R
companies_2014 = data.frame(py$company_2014)
names(companies_2014) = "Sentence"
companies_2014$Company <- sapply(str_extract_all(companies_2014$Sentence, "\\b[A-Z]+[a-z]*\\b"), paste, collapse= ' ')
companies_2014$Company <- trimws(companies_2014$Company)

#filter stopwords in Company column and small names
companies_2014 <- companies_2014 %>% filter(!(tolower(Company) %in% stopwords(kind = "en")) & 
                                    nchar(Company) > 3 & 
                                    !(grepl("[.,!?]", Company)) &
                                    !(grepl("America", Company))&
                                    str_count(Company, "\\s") < 3) #remove stop words, small names and non-letter character potential companies
companies$Company <- gsub("'s*|,$", "", companies$Company) #remove possessives or commas

#add predictors
companies_2014 <- companies_2014 %>%
                    mutate(correct = ifelse((Company %in% incs$V1 & !(Company %in% world.cities$country.etc)), 1, 0),        #is name represented in labels dataset?
                           board = ifelse(grepl("board|executive|manager|founder|chief|business|veteran", Sentence), 1, 0),  #business identifiers
                           stock = ifelse(grepl("stock|shares|performance|products", Sentence), 1, 0),                       #stock identifiers
                           verbs = ifelse(grepl("jump|rose|trad|rall|announc|spen|earn|slump|fall|miss", Sentence), 1, 0),   #company verb identifiers
                           location = ifelse(Company %in% world.cities$name | Company %in% world.cities$country.etc, 1, 0),
                           location2 = ifelse(grepl("River|Street|Place|Land|Drive|Avenue", Company), 1, 0),
                           name = ifelse(Company %in% common_names$name, 1, 0),
                           company = ifelse(grepl("Corp|Inc|Ltd|Management", Company), 1, 0),
                           dow = ifelse(grepl("Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday", Company), 1, 0),
                           month = ifelse(grepl("January|February|March|April|May|June|July|August|September|October|November|December", Company), 1, 0),
                           n_capitalized = str_count(Sentence, "[A-Z]"),
                           n_words = str_count(Sentence, "\\s"),
                           political = ifelse(grepl(" House| Representative| Republican| Democrat| Speaker| Congress| Senate| The | Fiscal| Congress| Central| Bank", Company), 1, 0))

#add final predictors
companies_2014 <- companies_2014 %>%
                    rowwise() %>%
                    mutate(preposition = ifelse(grepl(paste0("( to| with| at| by| from) ", Company), Sentence), 1, 0),
                           possession = ifelse(grepl(paste0(Company, "\'s"), Sentence), 1, 0),
                           comma = ifelse(grepl(paste0(Company, ","), Sentence), 1, 0))

# run predictions on test data
companies_2014$predictions <- predict(logit, companies_2014, type = "response")

# AUC analysis
roc_obj <- roc(companies_2014$correct, companies_2014$predictions)
auc(roc_obj)
plot(roc_obj)

# return predicted CEO names (arbitrary probability cutoff: 0.33)
predicted_companies <- companies_2014 %>%
                          filter(predictions > 0.33) %>%
                          arrange(-predictions) %>%
                          distinct(Company)
write.csv(predicted_companies, "company_matches.csv")
```

# Percentages

A different approach was made to subset percentages. As numbers can vary widely, I thought it better not to use labels, and use broad regex patterns instead, to catch a wide array of percentage expressions. So, in this case, no NER model was used. Instead, the following regex was used: ((?:\\S+\\s+)\{0,3\}\\b(\\S\{1,5\}%|(?:[0-9]\{1,3\}\.\*[0-9]\{0,2\}|half a\*|quarter a\*|\\S\{1,10\}) (?:percent|percentage points\*))\\b\\s\*(?:\\S+\\b\\s*){0,3})|. It matches sentences around the expression, as previously, but returns the match in a separate column. This returns x% as well as written declarations (five percent), or more peculiar expressions (a quarter percentage points). The list of distinct percent expressions matched can be seen in percentage_matches.csv.

```{python pctpatterns, include = FALSE}
os.chdir('C:\\Users\\enzop\\Desktop\\Enzo\\Northwestern\\IEMS308\\HW3\\BI-articles\\2013') #set your path

#use 2013 data as training
pct_event_list = []

#find each percentage-related snippet: output sentence (3 words before/after) and snippet as separate variable
for filename in os.listdir(os.getcwd()):
  text = open(filename,"r").read()
  pct_tuple_list = re.findall(r'((?:\S+\s+){0,3}\b(\S{1,5}%|(?:[0-9]{1,3}\.*[0-9]{0,2}|half a*|quarter a*|\S{1,10}) (?:percent|percentage points*))\b\s*(?:\S+\b\s*){0,3})', text)
  pct_event_list.append(pct_tuple_list)

#unlist items and add tuples to dataframe
pct_events = [item for sublist in pct_event_list for item in sublist]
pct_events = pd.DataFrame(pct_events, columns=["Sentence", "expression"])
```

```{r pctpredictors}
pct_events = py$pct_events

predicted_pct <- pct_events %>%
                   distinct(expression)
write.csv(predicted_pct, "percentage_matches.csv")
```

